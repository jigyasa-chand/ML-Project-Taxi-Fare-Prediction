{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 5a\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# scale larger positive and values to between -1,1 depending on the largest value in the data\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "df = pd.read_csv(\"J:\\MTech Notes\\FOML\\Assignments\\Assignment 4\\Train Set.csv\", header=0)\n",
    "df.columns = [\"grade1\",\"grade2\",\"label\"]\n",
    "\n",
    "X = df[[\"grade1\",\"grade2\"]]\n",
    "X = np.array(X)\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "Y = df['label']\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is  0.38695731163997166\n",
      "theta  [1.5141687796007266, 0.5159581367783731]\n"
     ]
    }
   ],
   "source": [
    "def Sigmoid(z):\n",
    "    G_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))\n",
    "    return G_of_Z \n",
    "##This hypothesis will be used to calculate each instance of the Cost Function\n",
    "def Hypothesis(theta, x):\n",
    "    z = -1\n",
    "    for i in range(len(theta)):\n",
    "        z += x[i]*theta[i]\n",
    "    return Sigmoid(z)\n",
    "\n",
    "##For each member of the dataset, the result (Y) determines which variation of the cost function is used\n",
    "##The Y = 0 cost function punishes high probability estimations, and the Y = 1 it punishes low scores\n",
    "##The \"punishment\" makes the change in the gradient of ThetaCurrent - Average(CostFunction(Dataset)) greater\n",
    "def Cost_Function(X,Y,theta,m):\n",
    "    sumOfErrors = 0\n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        hi = Hypothesis(theta,xi)\n",
    "        if Y[i] == 1:\n",
    "            error = Y[i] * math.log(hi)\n",
    "        elif Y[i] == 0:\n",
    "            error = (1-Y[i]) * math.log(1-hi)\n",
    "        sumOfErrors += error\n",
    "    const = -1/m\n",
    "    J = const * sumOfErrors\n",
    "    print ('cost is ', J )\n",
    "    return J\n",
    "\n",
    "##This function creates the gradient component for each Theta value \n",
    "def Cost_Function_Derivative(X,Y,theta,j,m,alpha):\n",
    "    sumErrors = 0\n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        xij = xi[j]\n",
    "        hi = Hypothesis(theta,X[i])\n",
    "        error = (hi - Y[i])*xij\n",
    "        sumErrors += error\n",
    "    m = len(Y)\n",
    "    constant = float(alpha)/float(m)\n",
    "    J = constant * sumErrors\n",
    "    return J\n",
    "\n",
    "def Gradient_Descent(X,Y,theta,m,alpha):\n",
    "    new_theta = []\n",
    "    constant = alpha/m\n",
    "    for j in range(len(theta)):\n",
    "        CFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha)\n",
    "        new_theta_value = theta[j] - CFDerivative\n",
    "        new_theta.append(new_theta_value)\n",
    "    return new_theta\n",
    "\n",
    "def Logistic_Regression(X,Y,alpha,theta,num_iters):\n",
    "    m = len(Y)\n",
    "    for x in range(num_iters):\n",
    "        new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
    "        theta = new_theta\n",
    "        if x % 100 == 0:\n",
    "#here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration\n",
    "            Cost_Function(X,Y,theta,m)\n",
    "    print ('theta ', theta)\t\n",
    "    \n",
    "#Answer 5b    \n",
    "initial_theta = [1.5,0.5]\n",
    "alpha = 0.1\n",
    "iterations = 1\n",
    "Logistic_Regression(X,Y,alpha,initial_theta,iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is  0.38695731163997166\n",
      "cost is  0.1674719163252521\n",
      "cost is  0.1038704434479708\n",
      "cost is  0.07520399503227515\n",
      "cost is  0.05905251946601321\n",
      "cost is  0.0487070607645565\n",
      "cost is  0.04151237807843458\n",
      "cost is  0.03621550967987534\n",
      "cost is  0.03214978245326757\n",
      "cost is  0.028928207374634102\n",
      "theta  [4.6206370851228264, 3.8181787780081735]\n"
     ]
    }
   ],
   "source": [
    "initial_theta = [1.5,0.5]\n",
    "alpha = 0.1\n",
    "iterations = 1000\n",
    "Logistic_Regression(X,Y,alpha,initial_theta,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is  0.38695731163997166\n",
      "cost is  0.1674719163252521\n",
      "cost is  0.1038704434479708\n",
      "cost is  0.07520399503227515\n",
      "cost is  0.05905251946601321\n",
      "cost is  0.0487070607645565\n",
      "cost is  0.04151237807843458\n",
      "cost is  0.03621550967987534\n",
      "cost is  0.03214978245326757\n",
      "cost is  0.028928207374634102\n",
      "theta  [4.6206370851228264, 3.8181787780081735]\n",
      "Prediction:  1\n",
      "Prediction:  0\n",
      "Prediction:  0\n",
      "Prediction:  1\n",
      "Prediction:  1\n",
      "Prediction:  0\n",
      "Your score:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#Answer 5 b (iii)\n",
    "def Logistic_Regression(X,Y,alpha,theta,num_iters):\n",
    "    m = len(Y)\n",
    "    for x in range(num_iters):\n",
    "        new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
    "        theta = new_theta\n",
    "        if x % 100 == 0:\n",
    "#here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration\n",
    "            Cost_Function(X,Y,theta,m)\n",
    "    print ('theta ', theta)\n",
    "    accuracy(theta)\n",
    "def accuracy(theta):\n",
    "    score = 0\n",
    "    length = len(X_test)\n",
    "    for i in range(length):\n",
    "        prediction = round(Hypothesis(X_test[i],theta))\n",
    "        answer = Y_test[i]\n",
    "        if prediction == answer:\n",
    "            score += 1\n",
    "        print ('Prediction: ', prediction)\n",
    "    my_score = float(score) / float(length)\n",
    "    \n",
    "    print ('Accuracy: ', my_score)\n",
    "    \n",
    "    \n",
    "test=pd.read_csv('J:\\MTech Notes\\FOML\\Assignments\\Assignment 4\\Test Set.csv')\n",
    "# clean up data\n",
    "test.columns = [\"grade1\",\"grade2\",\"label\"]\n",
    "X_test = test[[\"grade1\",\"grade2\"]]\n",
    "X_test = np.array(X_test)\n",
    "X_test = min_max_scaler.fit_transform(X_test)\n",
    "#Y = df[\"label\"].map(lambda x: float(x.rstrip(';')))\n",
    "Y_test= test['label']\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "\n",
    "\n",
    "initial_theta = [1.5,0.5]\n",
    "alpha = 0.1\n",
    "iterations = 1000\n",
    "Logistic_Regression(X,Y,alpha,initial_theta,iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  66.66666666666667\n",
      "Recall:  66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "y_actual=[1,0,0,1,1,0]\n",
    "\n",
    "def recall_precision(y_act, y_pred):\n",
    "    tp,fp,tn,fn=0,0,0,0\n",
    "    for i in range(len(y_act)):\n",
    "        if ((y_act[i] == 1) & (y_pred[i] == 1)):\n",
    "            tp += 1\n",
    "        if ((y_act[i] == 1) & (y_pred[i] == 0)):\n",
    "            fp += 1\n",
    "        if ((y_act[i] == 0) & (y_pred[i] == 1)):\n",
    "            fn += 1\n",
    "        if ((y_act[i] == 0) & (y_pred[i] == 0)):\n",
    "            tn += 1\n",
    "\n",
    "    \n",
    "    precision=(tp  * 100)/ float( tp + fp)\n",
    "    print(\"Precision: \",precision)\n",
    "    recall=(tp  * 100)/ float( tp + fn)\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "recall_precision(y_actual,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
